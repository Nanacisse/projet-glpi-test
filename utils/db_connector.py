import pyodbc
from sqlalchemy import create_engine, text
import pandas as pd
import numpy as np
from datetime import datetime
from sqlalchemy.exc import DBAPIError
import warnings

# Supprimer les warnings
warnings.filterwarnings('ignore')

# --- CONFIGURATION DE LA CONNEXION ---
SERVER_NAME = 'CIYGSG9030DK\\SQLEXPRESS' 
DATABASE_NAME = 'GLPI_DWH'
DRIVER = 'ODBC Driver 17 for SQL Server'

def get_db_connection_url():
    """Cr√©e l'URL de connexion pour SQLAlchemy."""
    return f"mssql+pyodbc://@{SERVER_NAME}/{DATABASE_NAME}?driver={DRIVER}&trusted_connection=yes"

def get_db_connection():
    """Cr√©e une connexion directe pyodbc."""
    try:
        conn_str = f'DRIVER={DRIVER};SERVER={SERVER_NAME};DATABASE={DATABASE_NAME};Trusted_Connection=yes;'
        return pyodbc.connect(conn_str)
    except Exception as e:
        print(f"Erreur de connexion pyodbc: {e}")
        return None

# Initialisation du moteur SQLAlchemy
try:
    engine = create_engine(get_db_connection_url(), connect_args={'autocommit': True}, pool_pre_ping=True)
    print("Connexion √† la base de donn√©es √©tablie")
except Exception as e:
    print(f"Erreur de connexion SQLAlchemy : {e}")
    engine = None

def load_categories_data():
    """Charge les cat√©gories depuis DimCategory."""
    if engine is None:
        return pd.DataFrame()
    
    try:
        query = text("""
        SELECT 
            CategoryID, 
            CategoryName, 
            ISNULL(CategoryFullName, CategoryName) AS Description,
            ParentCategoryID
        FROM DimCategory
        WHERE CategoryID IS NOT NULL
        ORDER BY CategoryName
        """)
        
        df = pd.read_sql(query, engine)
        print(f"{len(df)} cat√©gories charg√©es depuis DimCategory")
        return df
        
    except Exception as e:
        print(f"Erreur chargement cat√©gories: {e}")
        return pd.DataFrame()

def load_data_for_analysis():
    """Charge les donn√©es n√©cessaires pour l'analyse."""
    if engine is None:
        print("Erreur: Moteur de base de donn√©es non initialis√©.")
        return pd.DataFrame()
    
    try:
        query = text("""
        SELECT 
            FTP.FactKey,
            FTP.TicketID,
            FTP.AssigneeEmployeeKey,
            COALESCE(DE.UserFirstname + ' ' + DE.RealName, FTP.AssigneeFullName) AS AssigneeFullName,
            FTP.ProblemDescription,
            FTP.SolutionContent,
            FTP.ResolutionDurationSec,
            DD.FullDate AS DateCreation
        FROM FactTicketPerformance FTP
        JOIN DimDate DD ON FTP.DateCreationKey = DD.DateKey
        LEFT JOIN DimEmployee DE ON FTP.AssigneeEmployeeKey = DE.EmployeeKey
        WHERE FTP.ProblemDescription IS NOT NULL 
          AND FTP.SolutionContent IS NOT NULL
          AND FTP.ResolutionDurationSec IS NOT NULL
          AND FTP.AssigneeFullName IS NOT NULL
        ORDER BY DD.FullDate DESC
        """)
        
        df = pd.read_sql(query, engine)
        
        if not df.empty:
            df['TempsHeures'] = df['ResolutionDurationSec'] / 3600.0
            df['TempsHeures'] = df['TempsHeures'].round(2)
            
        print(f"Donn√©es charg√©es : {len(df)} tickets")
        return df
        
    except DBAPIError as e:
        print(f"Erreur de chargement des donn√©es: {e}")
        return pd.DataFrame()
    except Exception as e:
        print(f"Erreur de chargement des donn√©es: {e}")
        return pd.DataFrame()

def delete_old_data(conn):
    """Supprime les anciennes donn√©es des tables."""
    try:
        # Supprimer d'abord FactAnomaliesDetail (enfant) puis DimRecurrentProblems (parent)
        conn.execute(text("DELETE FROM FactAnomaliesDetail"))
        conn.execute(text("DELETE FROM DimRecurrentProblems"))
        
        print("Anciennes donn√©es supprim√©es")
        return True
    except Exception as e:
        print(f"‚ö† Erreur lors de la suppression des donn√©es : {e}")
        # Continuer m√™me si l'effacement √©choue
        return True

def save_analysis_results(df_anomalies: pd.DataFrame, cluster_results: pd.DataFrame):
    """Sauvegarde les r√©sultats d'analyse SANS TempsMoyenHeures."""
    if engine is None:
        print("Erreur: Moteur de base de donn√©es non initialis√©.")
        return False
        
    try:
        with engine.connect() as conn:
            
            # Supprimer les anciennes donn√©es
            delete_old_data(conn)
                
            if cluster_results is not None and not cluster_results.empty:
                # ‚≠ê‚≠ê CORRECTION : NE PAS INCLURE TempsMoyenHeures ‚≠ê‚≠ê
                # S√©lectionner uniquement les colonnes qui existent
                available_columns = ['ProblemNameGroup', 'ClusterID', 'KeywordMatch', 'RecurrenceCount', 'CategoryID']
                
                # V√©rifier quelles colonnes sont pr√©sentes dans cluster_results
                columns_to_use = [col for col in available_columns if col in cluster_results.columns]
                
                if len(columns_to_use) < 3:
                    print("‚ö† Pas assez de colonnes dans cluster_results")
                    columns_to_use = ['ProblemNameGroup', 'ClusterID', 'KeywordMatch']
                
                clusters_to_save = cluster_results[columns_to_use].copy()
                
                # S'assurer que ClusterID est unique et commence √† 1
                clusters_to_save['ClusterID'] = range(1, len(clusters_to_save) + 1)
                
                # G√©rer les valeurs NULL/Nan pour CategoryID si pr√©sent
                if 'CategoryID' in clusters_to_save.columns:
                    clusters_to_save['CategoryID'] = clusters_to_save['CategoryID'].replace({np.nan: None}).fillna(0)
                
                print(f"üíæ Pr√©paration {len(clusters_to_save)} clusters...")
                print(f"   Colonnes √† sauvegarder: {list(clusters_to_save.columns)}")
                
                # ‚≠ê‚≠ê NE PAS ajouter TempsMoyenHeures
                # La ligne suivante √©tait probl√©matique :
                # clusters_to_save['TempsMoyenHeures'] = cluster_times  # ‚¨ÖÔ∏è SUPPRIMER
                
                # Ins√©rer dans DimRecurrentProblems
                clusters_to_save.to_sql(
                    'DimRecurrentProblems', 
                    conn, 
                    if_exists='append', 
                    index=False
                )
                print(f"‚úÖ {len(clusters_to_save)} probl√®mes r√©currents sauvegard√©s dans DimRecurrentProblems")
                
                # Cr√©er un mapping pour mettre √† jour les ClusterID dans df_anomalies
                cluster_mapping = dict(zip(range(len(clusters_to_save)), clusters_to_save['ClusterID']))
                
                # Mettre √† jour les ClusterID dans df_anomalies
                if not df_anomalies.empty:
                    df_anomalies['ClusterID'] = df_anomalies['ClusterID'].map(cluster_mapping).fillna(0)
            
            if not df_anomalies.empty:
                # Pr√©parer les donn√©es pour FactAnomaliesDetail
                anomalies_to_save = df_anomalies[[
                    'TicketID', 'FactKey', 'AssigneeEmployeeKey', 'AssigneeFullName',
                    'TicketNote', 'EmployeeAvgScore', 'ScoreSemantique', 'NoteSemantique',
                    'ScoreConcordance', 'NoteConcordance', 'TempsHeures', 'NoteTemporelle',
                    'Statut', 'ClusterID', 'CategoryID'
                ]].copy()
                
                # Remplir les valeurs NaN
                anomalies_to_save = anomalies_to_save.fillna({
                    'TicketNote': 0, 'EmployeeAvgScore': 0, 
                    'ScoreSemantique': 0, 'NoteSemantique': 0,
                    'ScoreConcordance': 0, 'NoteConcordance': 0,
                    'TempsHeures': 0, 'NoteTemporelle': 0,
                    'Statut': 'Non D√©termin√©', 'ClusterID': 0,
                    'CategoryID': 0
                })
                
                # S'assurer que les types de donn√©es sont corrects
                anomalies_to_save['TempsHeures'] = pd.to_numeric(anomalies_to_save['TempsHeures'], errors='coerce').fillna(0)
                
                print(f"üíæ Pr√©paration {len(anomalies_to_save)} anomalies...")
                
                # Ins√©rer dans FactAnomaliesDetail
                anomalies_to_save.to_sql(
                    'FactAnomaliesDetail', 
                    conn, 
                    if_exists='append', 
                    index=False
                )
                print(f"‚úÖ {len(anomalies_to_save)} anomalies sauvegard√©es dans FactAnomaliesDetail")
            
            # Valider la transaction
            conn.commit()
            print("üéâ Sauvegarde termin√©e avec succ√®s!")
            return True
            
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde: {e}")
        import traceback
        traceback.print_exc()
        try:
            conn.rollback()
        except:
            pass
        return False