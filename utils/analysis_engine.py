import pandas as pd
import numpy as np
import spacy
from difflib import SequenceMatcher
from sklearn.cluster import AgglomerativeClustering
from sentence_transformers import SentenceTransformer
import re

#Initialisation des ressources IA/NLP
try:
    nlp = spacy.load("fr_core_news_sm")
    st_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
except Exception as e:
    print(f"Erreur de chargement des mod√®les NLP/IA : {e}")
    nlp = None
    st_model = None

#D√©finition des constantes d'anomalie
SEMAN_THRESHOLD = 0.60  # 60%
CONC_THRESHOLD = 0.40   # 40%
Z_SCORE_THRESHOLD = 2   # |Z| > 2

def calculate_semantique_score(text):
    """Calcule le score s√©mantique pour les descriptions de solutions."""
    if not nlp or pd.isna(text):
        return 0.0
    
    try:
        text_str = str(text)
        if len(text_str.strip()) == 0:
            return 0.0
            
        doc = nlp(text_str)
        
        #M√©triques de qualit√© s√©mantique
        total_tokens = len(doc)
        if total_tokens == 0:
            return 0.0
        
        #Longueur appropri√©e
        length_score = min(20, total_tokens * 0.5)
        
        #Structure (pr√©sence de phrases compl√®tes)
        sentences = list(doc.sents)
        num_sentences = len(sentences)
        structure_score = min(15, num_sentences * 3)
        
        #Richesse lexicale
        non_stop_words = [token for token in doc if token.is_alpha and not token.is_stop]
        lexical_richness = len(non_stop_words) / total_tokens if total_tokens > 0 else 0
        richness_score = lexical_richness * 30
        
        #Coh√©rence technique
        technical_terms = ['erreur', 'bug', 'probl√®me', 'solution', 'correct', 
                          'r√©parer', 'installer', 'configurer', 'r√©soudre', 'd√©panner',
                          'incident', 'panne', 'dysfonctionnement', 'technique']
        
        technical_count = sum(1 for token in doc if token.text.lower() in technical_terms)
        technical_score = min(20, technical_count * 2)
        
        #Clart√© (faible proportion de mots vides)
        stop_word_ratio = sum(1 for token in doc if token.is_stop) / total_tokens
        clarity_score = (1 - stop_word_ratio) * 15
        
        #Score total
        total_score = length_score + structure_score + richness_score + technical_score + clarity_score
        
        #P√©nalit√©s
        penalties = 0
        
        #P√©nalit√© pour mots inconnus
        unknown_words = sum(1 for token in doc if token.is_oov and token.is_alpha)
        penalties += min(10, unknown_words * 2)
        
        #P√©nalit√© pour phrases trop longues
        if sentences:
            avg_sentence_length = total_tokens / len(sentences)
            if avg_sentence_length > 25:
                penalties += 10
        
        final_score = max(0, min(100, total_score - penalties))
        return round(final_score, 2)
        
    except Exception as e:
        print(f"Erreur analyse s√©mantique: {e}")
        return 50.0

def calculate_concordance_score(problem, solution):
    """Calcule le score de concordance entre probl√®me et solution."""
    if pd.isna(problem) or pd.isna(solution):
        return 0.0
    
    try:
        problem_str = str(problem).lower().strip()
        solution_str = str(solution).lower().strip()
        
        if not problem_str or not solution_str:
            return 0.0
        
        #Similarit√© basique (SequenceMatcher)
        matcher = SequenceMatcher(None, problem_str, solution_str)
        base_similarity = matcher.ratio() * 40
        
        #Pr√©sence de mots-cl√©s de r√©solution
        resolution_keywords = ['r√©solu', 'corrig√©', 'r√©par√©', 'fix√©', 'solution', 
                              'r√©solution', 'termin√©', 'compl√©t√©', 'r√©ussi']
        
        resolution_found = any(keyword in solution_str for keyword in resolution_keywords)
        resolution_score = 20 if resolution_found else 0
        
        #Longueur relative de la solution
        problem_words = len(problem_str.split())
        solution_words = len(solution_str.split())
        
        if problem_words > 0 and solution_words > 0:
            length_ratio = min(1.0, solution_words / problem_words)
            length_score = length_ratio * 20
        else:
            length_score = 0
        
        #Structure de la solution
        solution_has_steps = any(marker in solution_str for marker in ['premi√®rement', 'ensuite', 'puis', '√©tape', 'step'])
        structure_score = 10 if solution_has_steps else 5
        
        #Pr√©sence d'indicateurs de compl√©tion
        completion_indicators = any(marker in solution_str for marker in ['termin√©', 'fini', 'compl√©t√©', 'finalis√©'])
        completion_score = 10 if completion_indicators else 0
        
        total_score = base_similarity + resolution_score + length_score + structure_score + completion_score
        return min(100, round(total_score, 2))
        
    except Exception as e:
        print(f"Erreur calcul concordance: {e}")
        return 50.0

def calculate_temporal_score(df):
    """Calcule le Z-score temporel et d√©tecte les anomalies temporelles."""
    if df.empty:
        return df
        
    #Calcul des statistiques temporelles
    mean_h = df['TempsHeures'].mean()
    std_h = df['TempsHeures'].std()
    
    df['TempsMoyenHeures'] = round(mean_h, 2)
    df['EcartTypeHeures'] = round(std_h if std_h > 0 else 1.0, 2)
    
    #Calcul du Z-score
    df['ScoreTemporel'] = (df['TempsHeures'] - mean_h) / df['EcartTypeHeures']
    
    #D√©tection d'anomalie temporelle
    df['AnomalieTemporelle'] = np.where(np.abs(df['ScoreTemporel']) > Z_SCORE_THRESHOLD, 'Oui', 'Non')
    
    return df

def determine_final_status(row):
    """D√©termine le statut final bas√© sur les 3 scores."""
    sem_ok = row['ScoreSemantique'] >= SEMAN_THRESHOLD * 100
    conc_ok = row['ScoreConcordance'] >= CONC_THRESHOLD * 100
    temp_ok = row['AnomalieTemporelle'] == 'Non'
    
    #Logique des statuts
    if sem_ok and conc_ok and temp_ok:
        return 'OK'
    elif sem_ok and conc_ok and not temp_ok:
        return 'Anomalie de Temps'
    elif sem_ok and not conc_ok and temp_ok:
        return 'Anomalie de Concordance'
    elif not sem_ok and conc_ok and temp_ok:
        return 'Anomalie S√©mantique'
    
    #Cas d'anomalies multiples
    num_anomalies = sum([not sem_ok, not conc_ok, not temp_ok])
    
    if num_anomalies >= 2:
        return 'Multiples Anomalies'
    
    return 'Anomalie Ind√©termin√©e'

def calculate_ticket_note(row):
    """Calcule la Note de Ticket (Base 10) par p√©nalit√©."""
    status = row['Statut']
    
    if status == 'OK':
        return 10.0
    elif status == 'Anomalie de Temps':
        return 7.0
    elif status == 'Multiples Anomalies':
        return 5.0
    elif status == 'Anomalie S√©mantique':
        return 8.0
    elif status == 'Anomalie de Concordance':
        return 8.0
    else:
        return 6.0

def generate_anomaly_description(row):
    """G√©n√®re une description de l'anomalie bas√©e sur les scores."""
    anomalies = []
    
    if row['ScoreSemantique'] < SEMAN_THRESHOLD * 100:
        anomalies.append("Description du solution peu claire")
    
    if row['ScoreConcordance'] < CONC_THRESHOLD * 100:
        anomalies.append("Solution peu pertinente par rapport au probl√®me")
    
    if row['AnomalieTemporelle'] == 'Oui':
        anomalies.append("Temps de r√©solution anormal")
    
    if anomalies:
        return "; ".join(anomalies)
    else:
        return "Aucune anomalie d√©tect√©e"

def run_full_analysis(df):
    """Ex√©cute l'int√©gralit√© du pipeline d'analyse IA."""
    if df.empty:
        return df, None
    
    print(f"D√©but de l'analyse sur {len(df)} tickets assign√©s")
    
    #Analyse S√©mantique
    df['ScoreSemantique'] = df['SolutionContent'].apply(calculate_semantique_score)
    
    #Analyse de Concordance
    df['ScoreConcordance'] = df.apply(
        lambda row: calculate_concordance_score(row['ProblemDescription'], row['SolutionContent']),
        axis=1
    )
    
    #Analyse Temporelle
    df = calculate_temporal_score(df.copy())
    
    #D√©termination du Statut Final
    df['Statut'] = df.apply(determine_final_status, axis=1)
    
    #Calcul de la Note de Ticket
    df['TicketNote'] = df.apply(calculate_ticket_note, axis=1)
    
    #Calcul de la Moyenne Employ√©
    if 'AssigneeEmployeeKey' in df.columns:
        employee_avg = df.groupby('AssigneeEmployeeKey')['TicketNote'].mean().round(2)
        df['EmployeeAvgScore'] = df['AssigneeEmployeeKey'].map(employee_avg)
        df['EmployeeAvgScore'] = df['EmployeeAvgScore'].fillna(df['TicketNote'])
    else:
        df['EmployeeAvgScore'] = df['TicketNote']
    
    #G√©n√©ration de la description d'anomalie
    df['AnomalyDescription'] = df.apply(generate_anomaly_description, axis=1)
    
    # Clustering pour probl√®mes r√©currents - DANS LA FONCTION
    cluster_results = None
    if st_model is not None and 'ProblemDescription' in df.columns:
        try:
            descriptions = df['ProblemDescription'].astype(str).tolist()
            if descriptions:
                print(f"üîß D√©but du clustering sur {len(descriptions)} tickets...")
                
                # Encodage s√©mantique
                embeddings = st_model.encode(descriptions, show_progress_bar=False)
                
                # Calcul dynamique des clusters
                base_tickets = len(descriptions)
                if base_tickets <= 100:
                    n_clusters = max(3, base_tickets // 10)
                elif base_tickets <= 1000:
                    n_clusters = min(30, max(10, base_tickets // 25))
                else:
                    n_clusters = min(60, max(20, base_tickets // 50))
                
                print(f"üìä {n_clusters} clusters d√©termin√©s")
                
                # Clustering
                clustering_model = AgglomerativeClustering(
                    n_clusters=n_clusters,
                    metric='cosine',
                    linkage='average'
                )
                cluster_labels = clustering_model.fit_predict(embeddings)
                df['ClusterID'] = cluster_labels
                
                # Analyse s√©mantique automatique
                def extract_cluster_info(descriptions):
                    if not descriptions:
                        return "Sans description", "Aucun contenu"
                    
                    import re
                    from collections import Counter
                    import string
                    
                    # Nettoyage du texte
                    all_text = " ".join(descriptions).lower()
                    translator = str.maketrans('', '', string.punctuation + string.digits)
                    clean_text = all_text.translate(translator)
                    words = clean_text.split()
                    
                    # Filtrage des mots significatifs
                    stop_words = {
                        'bonjour', 'merci', 'cordialement', 'salut', 'hello',
                        'probl√®me', 'erreur', 'incident', 'souci', 'bug', 'panne',
                        'suite', 'depuis', 'quelque', 'plusieurs', 'chaque'
                    }
                    
                    meaningful_words = [
                        word for word in words 
                        if word not in stop_words and len(word) >= 4
                    ]
                    
                    # Extraction des mots-cl√©s fr√©quents
                    if meaningful_words:
                        word_freq = Counter(meaningful_words)
                        min_occurrences = max(2, len(descriptions) // 10)
                        common_words = [
                            word for word, count in word_freq.most_common(8) 
                            if count >= min_occurrences
                        ]
                        
                        if common_words:
                            group_name = f"Probl√®mes {', '.join(common_words[:3])}"
                            keywords = ", ".join(common_words[:5])
                            return group_name, keywords
                    
                    # Description par d√©faut
                    sample_desc = descriptions[len(descriptions) // 2] if len(descriptions) > 1 else descriptions[0]
                    clean_desc = sample_desc.strip()
                    if len(clean_desc) > 70:
                        clean_desc = clean_desc[:70] + "..."
                    
                    return clean_desc, "probl√®me technique"
                
                # G√©n√©ration des r√©sultats
                cluster_data = []
                for cluster_id in range(n_clusters):
                    cluster_descriptions = df[df['ClusterID'] == cluster_id]['ProblemDescription'].tolist()
                    if cluster_descriptions:
                        group_name, keywords = extract_cluster_info(cluster_descriptions)
                        cluster_data.append({
                            'ProblemNameGroup': group_name,
                            'ClusterID': cluster_id,
                            'KeywordMatch': keywords,
                            'RecurrenceCount': len(cluster_descriptions)
                        })
                
                cluster_results = pd.DataFrame(cluster_data)
                print(f"‚úÖ Clustering termin√©: {len(cluster_data)} clusters g√©n√©r√©s")
                
        except Exception as e:
            print(f"‚ùå Erreur clustering: {e}")

    print(f"Analyse termin√©e: {len(df)} tickets analys√©s")
    return df, cluster_results