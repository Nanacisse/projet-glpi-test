import pandas as pd
import numpy as np
import spacy
from sklearn.cluster import AgglomerativeClustering
from sentence_transformers import SentenceTransformer
import re
from collections import Counter
import string
import language_tool_python
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Tuple
import warnings
import time
import subprocess

# Supprimer les warnings
warnings.filterwarnings('ignore')

# --- Configuration et Initialisation ---

# D√©finition des constantes d'anomalie
SEMAN_THRESHOLD = 0.80  # 80% pour la s√©mantique
CONC_THRESHOLD = 0.20   # 20% pour la concordance
SLA_THRESHOLD = 4.0     # 4 heures pour le SLA

# --- CONSTANTES DE CLUSTERING INTELLIGENTES ---
MAX_TOTAL_CLUSTERS = 60            # Maximum ABSOLU (jamais d√©passer)
IDEAL_TICKETS_PER_CLUSTER = 55     # Cible: ~55 tickets par cluster (3,500/60 ‚âà 58)
MIN_TICKETS_PER_CLUSTER = 40       # Minimum pour un cluster significatif
MAX_TICKETS_PER_CLUSTER = 80       # Maximum avant de diviser
MIN_CLUSTER_SIZE = 3               # Minimum tickets pour cr√©er un cluster
MAX_CATEGORIES_TO_USE = 25         # Maximum cat√©gories DimCategory √† utiliser

# Initialisation des ressources
nlp = None
st_model = None
tool = None

print("Chargement des mod√®les NLP...")
try:
    nlp = spacy.load("fr_core_news_sm")
    st_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
    
    # V√©rifier si Java est disponible pour language_tool
    try:
        result = subprocess.run(['java', '-version'], capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            tool = language_tool_python.LanguageTool('fr', timeout=30)
            print("‚úì V√©rification grammaticale activ√©e avec Java")
        else:
            print("‚ö† Java non disponible - V√©rification grammaticale d√©sactiv√©e")
            tool = None
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as java_error:
        print(f"‚ö† Java non d√©tect√©, d√©sactivation de la v√©rification grammaticale: {java_error}")
        tool = None
    
    print("‚úì Mod√®les NLP charg√©s avec succ√®s")
    
except Exception as e:
    print(f"‚ö† Erreur de chargement des mod√®les: {e}")
    print("Continuer avec les fonctionnalit√©s de base...")
    nlp = None
    st_model = None
    tool = None

# --- Nouvelle analyse s√©mantique ---

def detect_vague_words_automatically(text: str, doc) -> List[str]:
    """
    D√©tecte automatiquement les mots vagues dans un texte.
    Utilise la grammaire et le contexte pour identifier les mots vagues.
    """
    vague_words = []
    
    try:
        modal_verbs = ['pouvoir', 'devoir', 'falloir', 'vouloir', 'sembler']
        
        uncertainty_adverbs = ['peut-√™tre', 'probablement', '√©ventuellement', 
                              'possiblement', 'apparemment', 'normalement',
                              'habituellement', 'g√©n√©ralement', 'souvent']
        
        generic_verbs = ['faire', 'mettre', 'prendre', 'voir', 'dire', 
                        'donner', 'rendre', 'laisser', 'passer']
        
        for token in doc:
            if token.lemma_ in modal_verbs and len(list(token.children)) < 2:
                vague_words.append(token.text)
            elif token.text.lower() in uncertainty_adverbs:
                vague_words.append(token.text)
            elif (token.pos_ == 'VERB' and token.lemma_ in generic_verbs and
                  not any(child.dep_ == 'obj' for child in token.children)):
                vague_words.append(token.text)
        
        sentences = list(doc.sents)
        for sent in sentences:
            words = [token.text.lower() for token in sent if token.is_alpha]
            unique_words = set(words)
            
            if len(words) < 8 and len(unique_words) < 6:
                vague_words.append("phrase_generique")
        
        return list(set(vague_words))
        
    except Exception as e:
        print(f"Erreur d√©tection mots vagues: {e}")
        return []

def detect_structural_elements_automatically(doc) -> int:
    """
    D√©tecte automatiquement les √©l√©ments de structure dans un texte.
    Retourne le nombre d'√©tapes identifi√©es.
    """
    try:
        etapes_count = 0
        sentences = list(doc.sents)
        
        if not sentences:
            return 0
        
        numerical_markers = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10',
                            'premier', 'deuxi√®me', 'troisi√®me', 'quatri√®me',
                            'premi√®rement', 'deuxi√®mement', 'troisi√®mement']
        
        temporal_markers = ['ensuite', 'puis', 'apr√®s', 'alors', 'maintenant',
                           'finalement', 'enfin', 'ensuite']
        
        logical_markers = ['d\'abord', 'premi√®re √©tape', 'deuxi√®me √©tape',
                          '√©tape suivante', 'derni√®re √©tape', '√©tape finale']
        
        conditional_patterns = [r'si .* alors', r'lorsque .* donc',
                               r'apr√®s avoir .* ensuite']
        
        for sent in sentences:
            sent_text = sent.text.lower()
            
            for marker in numerical_markers + temporal_markers + logical_markers:
                if marker in sent_text:
                    etapes_count += 1
                    break
            
            for pattern in conditional_patterns:
                if re.search(pattern, sent_text):
                    etapes_count += 1
                    break
        
        return min(4, etapes_count)
        
    except Exception as e:
        print(f"Erreur d√©tection structure: {e}")
        return 0

def calculate_semantique_score(text):
    """
    Calcule le score s√©mantique selon les 4 crit√®res:
    1. Longueur des phrases (30 points)
    2. Structure logique (20 points) - D√âTECTION AUTOMATIQUE
    3. Qualit√© grammaticale (30 points)
    4. D√©tection des mots vagues (20 points) - D√âTECTION AUTOMATIQUE
    Total: 100 points convertis en pourcentage
    """
    if pd.isna(text) or not isinstance(text, str):
        return 0.0
    
    text_str = str(text).strip()
    if not text_str:
        return 0.0
    
    try:
        doc = nlp(text_str)
        sentences = list(doc.sents)
        
        if not sentences:
            return 0.0
        
        longueur_score = 30
        for sent in sentences:
            word_count = len([token for token in sent if not token.is_punct])
            if word_count > 25:
                longueur_score -= 5
                break
        
        etapes_trouvees = detect_structural_elements_automatically(doc)
        structure_score = etapes_trouvees * 5
        structure_score = min(20, structure_score)
        
        grammaire_score = 30
        if tool:
            try:
                matches = tool.check(text_str)
                nb_fautes = len(matches)
                nb_mots = len([token for token in doc if token.is_alpha])
                if nb_mots > 0:
                    taux_fautes = nb_fautes / nb_mots
                    grammaire_score = 30 * (1 - min(taux_fautes, 1))
            except Exception as e:
                print(f"‚ö† V√©rification grammaticale √©chou√©e: {e}")
                grammaire_score = 25
        else:
            grammaire_score = 25
        
        mots_vagues_trouves = detect_vague_words_automatically(text_str, doc)
        vague_score = 20 - (len(mots_vagues_trouves) * 4)
        vague_score = max(0, vague_score)
        
        total_points = longueur_score + structure_score + grammaire_score + vague_score
        pourcentage = (total_points / 100) * 100
        
        return min(100, round(pourcentage, 2))
        
    except Exception as e:
        print(f"Erreur analyse s√©mantique: {e}")
        return 50.0

def calculate_note_semantique(score_semantique):
    """Convertit le score s√©mantique (%) en note sur 10."""
    return round((score_semantique / 100) * 10, 2)

# --- Nouvelle analyse de concordance ---

def detect_resolution_keywords_automatically(solution_text: str, doc) -> bool:
    """
    D√©tecte automatiquement si la solution contient des mots-cl√©s de r√©solution.
    Retourne True si au moins un mot-cl√© de r√©solution est d√©tect√©.
    """
    try:
        solution_lower = solution_text.lower()
        
        resolution_patterns = [
            r'probl√®me (?:est|a √©t√©) (?:r√©solu|corrig√©|r√©par√©|fix√©)',
            r'(?:j\'ai|nous avons) (?:r√©solu|corrig√©|r√©par√©)',
            r'solution (?:est|a √©t√©) (?:trouv√©e|appliqu√©e|mise en ≈ìuvre)',
            r'ticket (?:est|a √©t√©) (?:cl√¥tur√©|ferm√©|termin√©)',
            r'incident (?:est|a √©t√©) (?:trait√©|r√©gl√©)'
        ]
        
        for pattern in resolution_patterns:
            if re.search(pattern, solution_lower):
                return True
        
        resolution_verbs = ['r√©soudre', 'corriger', 'r√©parer', 'fixer',
                           'terminer', 'cl√¥turer', 'traiter', 'r√©gler']
        
        for token in doc:
            if token.lemma_ in resolution_verbs and token.pos_ == 'VERB':
                children = list(token.children)
                if not any(child.dep_ == 'neg' for child in children):
                    return True
        
        resolution_nouns = ['solution', 'r√©solution', 'correction', 'r√©paration']
        
        for token in doc:
            if token.lemma_ in resolution_nouns and token.pos_ == 'NOUN':
                return True
        
        return False
        
    except Exception as e:
        print(f"Erreur d√©tection mots-cl√©s r√©solution: {e}")
        return False

def detect_completion_indicators_automatically(solution_text: str, doc) -> bool:
    """
    D√©tecte automatiquement si la solution contient des indicateurs de compl√©tion.
    Retourne True si au moins un indicateur est d√©tect√©.
    """
    try:
        solution_lower = solution_text.lower()
        
        completion_patterns = [
            r'(?:a √©t√©|est) (?:valid√©|v√©rifi√©|test√©|confirm√©)',
            r'(?:j\'ai|nous avons) (?:v√©rifi√©|test√©|valid√©)',
            r'(?:fonctionne|op√©rationnel|en marche) (?:correctement|normalement)',
            r'(?:mise en ≈ìuvre|impl√©mentation) (?:termin√©e|achev√©e|compl√®te)',
            r'(?:installation|configuration) (?:finalis√©e|achev√©e)'
        ]
        
        for pattern in completion_patterns:
            if re.search(pattern, solution_lower):
                return True
        
        completion_verbs = ['valider', 'v√©rifier', 'tester', 'confirmer',
                           'ex√©cuter', 'appliquer', 'impl√©menter', 'installer']
        
        for token in doc:
            if token.lemma_ in completion_verbs and token.pos_ == 'VERB':
                if token.morph.get('Tense') in ['Past', 'Pres']:
                    return True
        
        time_indicators = ['maintenant', 'actuellement', 'd√©sormais', '√† pr√©sent']
        
        for token in doc:
            if token.text.lower() in time_indicators:
                return True
        
        return False
        
    except Exception as e:
        print(f"Erreur d√©tection indicateurs compl√©tion: {e}")
        return False

def calculate_concordance_score(problem, solution):
    """
    Calcule le score de concordance selon les 3 crit√®res:
    1. Similarit√© s√©mantique (20 points)
    2. Mots-cl√©s de r√©solution (40 points) - D√âTECTION AUTOMATIQUE
    3. Indicateurs de compl√©tion (40 points) - D√âTECTION AUTOMATIQUE
    Total: 100 points convertis en pourcentage
    """
    if pd.isna(problem) or pd.isna(solution):
        return 0.0
    
    problem_str = str(problem).strip()
    solution_str = str(solution).strip()
    
    if not problem_str or not solution_str:
        return 0.0
    
    try:
        solution_doc = nlp(solution_str) if nlp else None
        
        similarite_score = 0
        if st_model and problem_str and solution_str:
            try:
                embeddings = st_model.encode([problem_str, solution_str])
                similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
                
                if similarity >= 0.65:
                    similarite_score = 20
                elif 0.50 <= similarity < 0.65:
                    similarite_score = 15
                elif 0.30 <= similarity < 0.50:
                    similarite_score = 10
                else:
                    similarite_score = 5
            except:
                similarite_score = 5
        
        resolution_detected = detect_resolution_keywords_automatically(solution_str, solution_doc)
        resolution_score = 40 if resolution_detected else 0
        
        completion_detected = detect_completion_indicators_automatically(solution_str, solution_doc)
        completion_score = 40 if completion_detected else 0
        
        total_points = similarite_score + resolution_score + completion_score
        pourcentage = (total_points / 100) * 100
        
        return min(100, round(pourcentage, 2))
        
    except Exception as e:
        print(f"Erreur calcul concordance: {e}")
        return 50.0

def calculate_note_concordance(score_concordance):
    """Convertit le score de concordance (%) en note sur 10."""
    return round((score_concordance / 100) * 10, 2)

# --- Nouvelle analyse temporelle ---

def calculate_temporal_note(temps_heures):
    """
    Calcule la note temporelle sur 10 selon le SLA:
    ‚â§ 4h : 10.0
    4-8h : 5.0
    8-24h : 3.0
    >24h : 2.0
    """
    if pd.isna(temps_heures):
        return 0.0
    
    if temps_heures <= 4.0:
        return 10.0
    elif 4.0 < temps_heures <= 8.0:
        return 5.0
    elif 8.0 < temps_heures <= 24.0:
        return 3.0
    else:
        return 2.0

# --- Calcul de la note finale sur 10 ---

def calculate_final_note(row):
    """
    Calcule la note finale sur 10 = Note Temporelle (50%) + Note S√©mantique (40%) + Note Concordance (10%)
    """
    note_temporelle = row.get('NoteTemporelle', 0)
    note_semantique = row.get('NoteSemantique', 0)
    note_concordance = row.get('NoteConcordance', 0)
    
    note_finale = (note_temporelle * 0.50) + (note_semantique * 0.40) + (note_concordance * 0.10)
    return round(note_finale, 2)

# --- Calcul de la moyenne employ√© ---

def calculate_employee_average(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calcule la moyenne des notes pour chaque employ√© selon la m√©thode demand√©e:
    1. Regroupement par employ√©
    2. Somme des notes pour chaque groupe
    3. Division par le nombre de tickets
    
    Formule: Moyenne Employ√© = Somme des notes / nombre de tickets
    """
    if 'AssigneeEmployeeKey' not in df.columns or 'TicketNote' not in df.columns:
        return df
    
    df_copy = df.copy()
    
    employee_stats = df_copy.groupby('AssigneeEmployeeKey').agg({
        'TicketNote': ['sum', 'count']
    }).reset_index()
    
    employee_stats.columns = ['AssigneeEmployeeKey', 'TotalNotes', 'TicketCount']
    
    employee_stats['EmployeeAvgScore'] = employee_stats.apply(
        lambda row: round(row['TotalNotes'] / row['TicketCount'], 2) if row['TicketCount'] > 0 else 0,
        axis=1
    )
    
    df_copy = df_copy.merge(
        employee_stats[['AssigneeEmployeeKey', 'EmployeeAvgScore']],
        on='AssigneeEmployeeKey',
        how='left'
    )
    
    return df_copy

# --- D√©termination du statut ---

def determine_final_status(row):
    """D√©termine le statut final bas√© sur les 3 indicateurs d'anomalie."""
    sem_ok = row['ScoreSemantique'] >= SEMAN_THRESHOLD * 100
    conc_ok = row['ScoreConcordance'] >= CONC_THRESHOLD * 100
    temp_ok = row['TempsHeures'] <= SLA_THRESHOLD
    
    if sem_ok and conc_ok and temp_ok:
        return 'OK'
    
    num_anomalies = sum([not sem_ok, not conc_ok, not temp_ok])
    
    if num_anomalies >= 2:
        return 'Multiples Anomalies'
    elif not temp_ok:
        return 'Anomalie de Temps'
    elif not conc_ok:
        return 'Anomalie de Concordance'
    elif not sem_ok:
        return 'Anomalie S√©mantique'
    
    return 'Anomalie Ind√©termin√©e'

# --- Clustering pour probl√®mes r√©currents ---

def extract_keywords_automatically(descriptions: List[str]) -> str:
    """Extrait automatiquement les mots-cl√©s les plus pertinents d'une liste de descriptions."""
    if not descriptions:
        return "Aucun mot-cl√©"
    
    try:
        all_text = ' '.join(descriptions)
        doc = nlp(all_text.lower())
        
        relevant_words = []
        for token in doc:
            if (token.pos_ in ['NOUN', 'VERB', 'ADJ'] and 
                not token.is_stop and 
                len(token.text) > 3 and
                token.text.isalpha()):
                relevant_words.append(token.lemma_)
        
        if relevant_words:
            word_counts = Counter(relevant_words)
            top_words = [word for word, count in word_counts.most_common(5)]
            return ', '.join(top_words)
        
        return "Aucun mot-cl√© significatif"
        
    except:
        return "Extraction √©chou√©e"

def generate_group_name_from_keywords(keywords: str) -> str:
    """G√©n√®re un nom de groupe bas√© sur les mots-cl√©s."""
    if not keywords or keywords == "Aucun mot-cl√©":
        return "Probl√®me Divers"
    
    first_keyword = keywords.split(',')[0].strip()
    
    if len(first_keyword) > 3:
        return f"Probl√®me de {first_keyword.capitalize()}"
    else:
        return "Probl√®me Technique"

def perform_advanced_clustering(df: pd.DataFrame, categories_data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Effectue le clustering avanc√© pour les probl√®mes r√©currents avec maximum 60 clusters R√âELS.
    """
    
    cluster_results = []
    df_with_clusters = df.copy()
    df_with_clusters['ClusterID'] = -1
    df_with_clusters['CategoryID'] = 0
    
    total_tickets = len(df)
    print(f"üìä D√©but clustering sur {total_tickets} tickets")
    
    try:
        # === √âTAPE 1: Clusters par cat√©gories DimCategory (UNIQUEMENT si significatifs) ===
        categories_used = 0
        if not categories_data.empty:
            print(f"üîç Recherche correspondance avec {len(categories_data)} cat√©gories DimCategory...")
            
            # Trier les cat√©gories par pertinence potentielle
            category_matches = []
            
            for idx, category_row in categories_data.iterrows():
                if categories_used >= MAX_CATEGORIES_TO_USE:
                    break
                    
                category_id = category_row['CategoryID']
                category_name = category_row['CategoryName']
                category_desc = str(category_row.get('Description', ''))
                
                matching_indices = []
                for ticket_idx, row in df.iterrows():
                    problem_desc = str(row.get('ProblemDescription', '')).lower()
                    solution_desc = str(row.get('SolutionContent', '')).lower()
                    
                    # Recherche dans probl√®me ET solution
                    if (category_name.lower() in problem_desc or 
                        category_name.lower() in solution_desc or
                        (category_desc and category_desc.lower() in problem_desc) or
                        (category_desc and category_desc.lower() in solution_desc)):
                        matching_indices.append(ticket_idx)
                
                if matching_indices:
                    category_matches.append({
                        'category_id': category_id,
                        'category_name': category_name,
                        'indices': matching_indices,
                        'count': len(matching_indices)
                    })
            
            # Trier par nombre de matches (d√©croissant)
            category_matches.sort(key=lambda x: x['count'], reverse=True)
            
            # Prendre les meilleures cat√©gories (celles avec le plus de tickets)
            for match in category_matches:
                if categories_used >= MAX_CATEGORIES_TO_USE:
                    break
                    
                if match['count'] >= MIN_CLUSTER_SIZE:  # Au moins 3 tickets
                    cluster_id = len(cluster_results)
                    
                    df_with_clusters.loc[match['indices'], 'ClusterID'] = cluster_id
                    df_with_clusters.loc[match['indices'], 'CategoryID'] = match['category_id']
                    
                    # Extraire les descriptions pour mots-cl√©s
                    descriptions = []
                    for idx in match['indices']:
                        if pd.notna(df.loc[idx, 'ProblemDescription']):
                            descriptions.append(str(df.loc[idx, 'ProblemDescription']))
                        if pd.notna(df.loc[idx, 'SolutionContent']):
                            descriptions.append(str(df.loc[idx, 'SolutionContent']))
                    
                    keywords = extract_keywords_automatically(descriptions)
                    
                    cluster_results.append({
                        'ProblemNameGroup': match['category_name'],
                        'ClusterID': cluster_id,
                        'KeywordMatch': keywords if keywords else match['category_name'],
                        'RecurrenceCount': match['count'],
                        'CategoryID': match['category_id']
                    })
                    
                    categories_used += 1
                    print(f"  ‚úì Cat√©gorie '{match['category_name']}': {match['count']} tickets")
        
        print(f"‚úÖ {categories_used} clusters cat√©gories cr√©√©s (min {MIN_CLUSTER_SIZE} tickets)")
        
        # === √âTAPE 2: Calcul intelligent du nombre de clusters n√©cessaires ===
        remaining_indices = df_with_clusters[df_with_clusters['ClusterID'] == -1].index.tolist()
        remaining_tickets = len(remaining_indices)
        
        print(f"üì¶ Tickets restants √† clusteriser: {remaining_tickets}")
        
        if remaining_tickets > 0:
            # Calcul du nombre optimal de clusters
            slots_available = MAX_TOTAL_CLUSTERS - len(cluster_results)
            
            # Calcul bas√© sur ratio id√©al
            clusters_by_ratio = remaining_tickets // IDEAL_TICKETS_PER_CLUSTER
            
            # Ajustement: prendre le minimum entre ratio et slots disponibles
            clusters_needed = min(clusters_by_ratio, slots_available)
            
            # Minimum de clusters si assez de tickets
            if remaining_tickets > 100 and clusters_needed < 5:
                clusters_needed = min(5, slots_available)
            
            # Maximum pour √©viter les clusters trop petits
            max_by_min_size = remaining_tickets // MIN_TICKETS_PER_CLUSTER
            clusters_needed = min(clusters_needed, max_by_min_size)
            
            print(f"üéØ Calcul clusters n√©cessaires:")
            print(f"   - Par ratio ({IDEAL_TICKETS_PER_CLUSTER} tickets/cluster): {clusters_by_ratio}")
            print(f"   - Slots disponibles: {slots_available}")
            print(f"   - Clusters d√©cid√©s: {clusters_needed}")
            print(f"   - Ratio final: {remaining_tickets/clusters_needed:.1f} tickets/cluster")
            
            # === √âTAPE 3: Clustering hi√©rarchique ===
            if clusters_needed >= 2 and st_model and remaining_tickets >= 10:
                print(f"üîó Clustering hi√©rarchique pour {remaining_tickets} tickets...")
                
                # Limite pratique pour performance
                MAX_TICKETS_FOR_CLUSTERING = 2500
                if remaining_tickets > MAX_TICKETS_FOR_CLUSTERING:
                    print(f"  ‚ö† √âchantillonnage √† {MAX_TICKETS_FOR_CLUSTERING} tickets")
                    # Prendre un √©chantillon repr√©sentatif
                    sample_indices = np.random.choice(
                        remaining_indices, 
                        size=MAX_TICKETS_FOR_CLUSTERING, 
                        replace=False
                    )
                    remaining_indices = sample_indices.tolist()
                    remaining_tickets = len(remaining_indices)
                
                # Pr√©parer les descriptions
                descriptions_to_cluster = []
                valid_indices = []
                
                for idx in remaining_indices:
                    problem_desc = str(df.loc[idx, 'ProblemDescription'])
                    if problem_desc and len(problem_desc.strip()) > 10:
                        descriptions_to_cluster.append(problem_desc)
                        valid_indices.append(idx)
                
                if len(descriptions_to_cluster) >= clusters_needed:
                    print(f"  üìù Encodage de {len(descriptions_to_cluster)} descriptions...")
                    embeddings = st_model.encode(descriptions_to_cluster, show_progress_bar=False)
                    
                    print(f"  üéØ Cr√©ation de {clusters_needed} clusters...")
                    
                    # Utiliser MiniBatchKMeans pour performance
                    try:
                        from sklearn.cluster import MiniBatchKMeans
                        clustering = MiniBatchKMeans(
                            n_clusters=clusters_needed,
                            random_state=42,
                            batch_size=1000,
                            n_init=3,
                            max_iter=100
                        )
                        cluster_labels = clustering.fit_predict(embeddings)
                        print(f"  ‚úÖ MiniBatchKMeans termin√©")
                    except Exception as km_error:
                        print(f"  ‚ö† MiniBatchKMeans √©chou√©, fallback √† AgglomerativeClustering")
                        clustering = AgglomerativeClustering(
                            n_clusters=clusters_needed,
                            metric='cosine',
                            linkage='average'
                        )
                        cluster_labels = clustering.fit_predict(embeddings)
                    
                    # Organiser les r√©sultats par cluster
                    cluster_groups = {}
                    for idx, cluster_label in zip(valid_indices, cluster_labels):
                        if cluster_label not in cluster_groups:
                            cluster_groups[cluster_label] = []
                        cluster_groups[cluster_label].append(idx)
                    
                    # Cr√©er les clusters R√âELS (uniquement si assez de tickets)
                    clusters_created = 0
                    for cluster_label, indices in cluster_groups.items():
                        if len(indices) >= MIN_CLUSTER_SIZE:  # Au moins 3 tickets
                            cluster_id = len(cluster_results)
                            
                            # V√©rifier limite absolue
                            if cluster_id >= MAX_TOTAL_CLUSTERS:
                                print(f"  ‚ö† Limite de {MAX_TOTAL_CLUSTERS} clusters atteinte")
                                break
                            
                            df_with_clusters.loc[indices, 'ClusterID'] = cluster_id
                            
                            # Extraire descriptions pour nom et mots-cl√©s
                            cluster_descriptions = []
                            for idx in indices:
                                if pd.notna(df.loc[idx, 'ProblemDescription']):
                                    cluster_descriptions.append(str(df.loc[idx, 'ProblemDescription']))
                            
                            keywords = extract_keywords_automatically(cluster_descriptions)
                            group_name = generate_group_name_from_keywords(keywords)
                            
                            # V√©rifier association avec cat√©gorie existante
                            cluster_category_id = 0
                            if categories_data is not None:
                                cluster_text = ' '.join(cluster_descriptions).lower()
                                for _, cat_row in categories_data.iterrows():
                                    cat_name = str(cat_row['CategoryName']).lower()
                                    if cat_name in cluster_text:
                                        cluster_category_id = cat_row['CategoryID']
                                        group_name = f"{cat_row['CategoryName']} ({group_name})"
                                        break
                            
                            cluster_results.append({
                                'ProblemNameGroup': group_name,
                                'ClusterID': cluster_id,
                                'KeywordMatch': keywords,
                                'RecurrenceCount': len(indices),
                                'CategoryID': cluster_category_id
                            })
                            
                            df_with_clusters.loc[indices, 'CategoryID'] = cluster_category_id
                            clusters_created += 1
                    
                    print(f"  ‚úÖ {clusters_created} clusters hi√©rarchiques cr√©√©s")
                else:
                    print(f"  ‚ö† Pas assez de descriptions valides pour clustering")
        
        # === √âTAPE 4: Gestion des tickets non clusteris√©s ===
        non_clustered = df_with_clusters[df_with_clusters['ClusterID'] == -1]
        if not non_clustered.empty:
            print(f"üìå {len(non_clustered)} tickets non clusteris√©s")
            
            # Si peu de tickets, les ajouter au cluster le plus proche
            if len(non_clustered) < 10 and len(cluster_results) > 0:
                # Trouver le cluster avec le plus de tickets
                largest_cluster_id = max(cluster_results, key=lambda x: x['RecurrenceCount'])['ClusterID']
                df_with_clusters.loc[non_clustered.index, 'ClusterID'] = largest_cluster_id
                print(f"  ‚ûï Ajout√©s au cluster #{largest_cluster_id}")
            elif len(non_clustered) >= MIN_CLUSTER_SIZE and len(cluster_results) < MAX_TOTAL_CLUSTERS:
                # Cr√©er un cluster "Divers"
                cluster_id = len(cluster_results)
                df_with_clusters.loc[non_clustered.index, 'ClusterID'] = cluster_id
                
                cluster_results.append({
                    'ProblemNameGroup': 'Probl√®mes Divers',
                    'ClusterID': cluster_id,
                    'KeywordMatch': 'Non classifi√©',
                    'RecurrenceCount': len(non_clustered),
                    'CategoryID': -1
                })
                print(f"  ‚úÖ Cluster 'Divers' cr√©√© avec {len(non_clustered)} tickets")
            else:
                # Distribuer parmi les clusters existants
                for idx in non_clustered.index:
                    # Trouver le cluster avec le moins de tickets
                    if cluster_results:
                        smallest_cluster = min(cluster_results, key=lambda x: x['RecurrenceCount'])
                        df_with_clusters.loc[idx, 'ClusterID'] = smallest_cluster['ClusterID']
                        smallest_cluster['RecurrenceCount'] += 1
        
        # === √âTAPE 5: Finalisation ===
        # Convertir en DataFrame
        cluster_results_df = pd.DataFrame(cluster_results)
        
        if not cluster_results_df.empty:
            # Trier par nombre d'occurrences
            cluster_results_df = cluster_results_df.sort_values('RecurrenceCount', ascending=False)
            
            # R√©assigner les IDs de 0 √† N-1
            cluster_results_df = cluster_results_df.reset_index(drop=True)
            cluster_results_df['ClusterID'] = range(len(cluster_results_df))
            
            # Mettre √† jour les IDs dans df_with_clusters
            id_mapping = {}
            for new_id, row in cluster_results_df.iterrows():
                old_id = row['ClusterID']
                id_mapping[old_id] = new_id
            
            df_with_clusters['ClusterID'] = df_with_clusters['ClusterID'].map(id_mapping)
        
        # Statistiques finales
        clustered_tickets = len(df_with_clusters[df_with_clusters['ClusterID'] != -1])
        final_cluster_count = len(cluster_results_df)
        
        print(f"\nüìä R√âSULTATS FINAUX DU CLUSTERING:")
        print(f"   ‚úÖ Clusters totaux: {final_cluster_count}")
        print(f"   ‚úÖ Tickets clusteris√©s: {clustered_tickets}/{total_tickets} ({clustered_tickets/total_tickets*100:.1f}%)")
        print(f"   ‚úÖ Ratio moyen: {clustered_tickets/max(1, final_cluster_count):.1f} tickets/cluster")
        
        if final_cluster_count > 0:
            avg_size = cluster_results_df['RecurrenceCount'].mean()
            min_size = cluster_results_df['RecurrenceCount'].min()
            max_size = cluster_results_df['RecurrenceCount'].max()
            print(f"   üìà Taille clusters: min={min_size}, max={max_size}, avg={avg_size:.1f}")
        
        # V√©rifier limite
        if final_cluster_count > MAX_TOTAL_CLUSTERS:
            print(f"‚ö† ATTENTION: {final_cluster_count} clusters > limite {MAX_TOTAL_CLUSTERS}")
            print(f"   Troncature √† {MAX_TOTAL_CLUSTERS} clusters...")
            cluster_results_df = cluster_results_df.head(MAX_TOTAL_CLUSTERS)
        
        print(f"üéØ OBJECTIF ATTEINT: {len(cluster_results_df)} clusters (max {MAX_TOTAL_CLUSTERS})")
        
        return cluster_results_df, df_with_clusters
        
    except Exception as e:
        print(f"‚ùå Erreur clustering avanc√©: {e}")
        import traceback
        traceback.print_exc()
        
        # Solution de repli: un seul cluster
        df_with_clusters['ClusterID'] = 0
        df_with_clusters['CategoryID'] = 0
        
        default_cluster = pd.DataFrame([{
            'ProblemNameGroup': 'Tous les Probl√®mes',
            'ClusterID': 0,
            'KeywordMatch': 'Erreur de clustering',
            'RecurrenceCount': len(df),
            'CategoryID': 0
        }])
        
        return default_cluster, df_with_clusters

# --- Fonction principale du pipeline ---

def run_full_analysis(df):
    """Ex√©cute l'int√©gralit√© du pipeline d'analyse IA avec optimisation."""
    if df.empty:
        print("DataFrame vide re√ßu")
        return pd.DataFrame(), pd.DataFrame()
    
    print(f"üöÄ D√©but de l'analyse sur {len(df)} tickets assign√©s")
    start_time = time.time()
    
    df['ClusterID'] = 0
    df['CategoryID'] = 0
    
    print("üìù Calcul des scores s√©mantiques...")
    # Traitement optimis√© par lots
    batch_size = 500
    scores_semantiques = []
    
    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i+batch_size]
        batch_scores = batch['SolutionContent'].apply(calculate_semantique_score)
        scores_semantiques.extend(batch_scores)
        if i % 1500 == 0 and i > 0:
            print(f"  Progression: {i}/{len(df)} tickets")
    
    df['ScoreSemantique'] = scores_semantiques
    df['NoteSemantique'] = df['ScoreSemantique'].apply(calculate_note_semantique)
    
    print("üìù Calcul des scores de concordance...")
    scores_concordance = []
    
    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i+batch_size]
        batch_scores = batch.apply(
            lambda row: calculate_concordance_score(row['ProblemDescription'], row['SolutionContent']),
            axis=1
        )
        scores_concordance.extend(batch_scores)
        if i % 1500 == 0 and i > 0:
            print(f"  Progression: {i}/{len(df)} tickets")
    
    df['ScoreConcordance'] = scores_concordance
    df['NoteConcordance'] = df['ScoreConcordance'].apply(calculate_note_concordance)
    
    print("‚è±Ô∏è Calcul des notes temporelles...")
    df['NoteTemporelle'] = df['TempsHeures'].apply(calculate_temporal_note)
    
    print("üßÆ Calcul des notes finales...")
    df['TicketNote'] = df.apply(calculate_final_note, axis=1)
    
    print("üè∑Ô∏è D√©termination des statuts...")
    df['Statut'] = df.apply(determine_final_status, axis=1)
    
    print("üë• Calcul des moyennes employ√©...")
    df = calculate_employee_average(df)
    
    print("üîó Clustering avanc√© en cours...")
    cluster_results = pd.DataFrame()
    try:
        from utils.db_connector import load_categories_data
        categories_data = load_categories_data()
        cluster_results, df_with_clusters = perform_advanced_clustering(df, categories_data)
        
        df['ClusterID'] = df_with_clusters['ClusterID']
        df['CategoryID'] = df_with_clusters['CategoryID']
        
    except Exception as e:
        print(f"‚ö† Erreur clustering: {e}")
        cluster_results = pd.DataFrame()
    
    print("üìä Pr√©paration des r√©sultats...")
    df_anomalies = df[[
        'TicketID', 'FactKey', 'AssigneeEmployeeKey', 'AssigneeFullName',
        'TicketNote', 'EmployeeAvgScore', 
        'ScoreSemantique', 'NoteSemantique',
        'ScoreConcordance', 'NoteConcordance',
        'TempsHeures', 'NoteTemporelle',
        'Statut', 'ClusterID', 'CategoryID'
    ]].copy()
    
    # Nettoyage des donn√©es num√©riques
    numeric_cols = ['TicketNote', 'EmployeeAvgScore', 'NoteSemantique', 'NoteConcordance', 'NoteTemporelle']
    for col in numeric_cols:
        if col in df_anomalies.columns:
            df_anomalies[col] = pd.to_numeric(df_anomalies[col], errors='coerce').fillna(0).round(2)
    
    # Calcul du temps d'ex√©cution
    end_time = time.time()
    execution_time = end_time - start_time
    minutes = int(execution_time // 60)
    seconds = int(execution_time % 60)
    
    print(f"\n‚úÖ ‚úÖ ‚úÖ ANALYSE TERMIN√âE AVEC SUCC√àS!")
    print(f"   ‚è±Ô∏è Temps total: {minutes}m {seconds}s")
    print(f"   üìä Tickets analys√©s: {len(df_anomalies)}")
    print(f"   üîó Clusters cr√©√©s: {len(cluster_results)} (max {MAX_TOTAL_CLUSTERS})")
    
    return df_anomalies, cluster_results